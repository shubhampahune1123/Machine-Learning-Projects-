# -*- coding: utf-8 -*-
"""signature_verification_shubham_Pahune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17US6UeJq5dtGuGUyhWyU0hndoXSrkmDF

# Problem Statement

## ***Signature Verification***

### **Problem Description:**
The task at hand is to create a machine learning model capable of distinguishing between genuine and forged handwritten signatures. Given a dataset of signature images, the model should be trained to return the match percentage between the two input signatures.

#### *Dataset Link: [Signature Dataset](https://www.kaggle.com/datasets/ishanikathuria/handwritten-signature-datasets/data)*

### **Key tasks**
1.   Data Preprocessing: Use the data provided in the above link to split it into training and validation sets.
2.  Model Architecture: Design a convolutional neural network (CNN) architecture suitable for signature verification. Experiment with different layers and hyperparameters to optimize model performance.
3.  Evaluation: Evaluate the model's performance using the validation dataset. Use relevant metrics such as accuracy, precision, recall, and F1-score.

## Libraries and Data/Model/Other Files
"""

import time
import copy
import numpy as np
import pandas as pd
from datetime import datetime
from statistics import mean

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.utils.data import SubsetRandomSampler

import torchvision
import torchvision.models as models
import torchvision.transforms as transforms

from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

from PIL import Image
from itertools import combinations

import random
from tqdm import tqdm # Progress Bar

import warnings
# Ignore all warnings
warnings.filterwarnings("ignore")
from statistics import mean

import cv2

import itertools
import random
import os

from sklearn.metrics import classification_report, confusion_matrix

# Move the processed image to a GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

device

root_data_path = "Downloads/archive"
train_path = root_data_path + "/CEDAR/CEDAR"
val_path = root_data_path + "/CEDAR/CEDAR"
test_path = root_data_path + "/CEDAR/CEDAR"

"""## Data Analysis

### Image Samples
"""

img = plt.imread('Downloads/archive/BHSig260-Hindi/BHSig260-Hindi/1/H-S-1-F-01.tif')
plt.imshow(img)

img = plt.imread('Downloads/archive/BHSig260-Hindi/BHSig260-Hindi/1/H-S-1-G-01.tif')
plt.imshow(img)

img = plt.imread('Downloads/archive/BHSig260-Bengali/BHSig260-Bengali/1/B-S-1-F-01.tif')
plt.imshow(img)

img = plt.imread('Downloads/archive/BHSig260-Bengali/BHSig260-Bengali/1/B-S-1-G-01.tif')
plt.imshow(img)

img = plt.imread('Downloads/archive/CEDAR/CEDAR/1/forgeries_1_1.png')
plt.imshow(img)

img = plt.imread('Downloads/archive/CEDAR/CEDAR/1/original_1_2.png')
plt.imshow(img)

"""### Analysis"""

import os

directory_path = "Downloads/archive/BHSig260-Hindi/BHSig260-Hindi/1"
files = os.listdir(directory_path)

for file in files:
    print(file)

import os

directory_path = "Downloads/archive/BHSig260-Bengali/BHSig260-Bengali/1"
files = os.listdir(directory_path)

for file in files:
    print(file)

import os

directory_path = "Downloads/archive/CEDAR/CEDAR/1"
files = os.listdir(directory_path)

for file in files:
    print(file)

import os

# Define the root path and the datasets of interest
root_data_path = "Downloads/archive"

# Function to print all files in a directory for verification
def print_files(directory_path):
    files = os.listdir(directory_path)
    for file in files:
        print(file)

# Verify files in each directory
print("CEDAR Files:")
print_files(os.path.join(root_data_path, "CEDAR", "CEDAR", "1"))

print("\nBHSig260-Hindi Files:")
print_files(os.path.join(root_data_path, "BHSig260-Hindi", "BHSig260-Hindi", "1"))

print("\nBHSig260-Bengali Files:")
print_files(os.path.join(root_data_path, "BHSig260-Bengali", "BHSig260-Bengali", "1"))

# Initialize lists to store paths of forgery and original images for each dataset
bengali_forgeries_imgs = []
bengali_original_imgs = []

hindi_forgeries_imgs = []
hindi_original_imgs = []

cedar_forgeries_imgs = []
cedar_original_imgs = []

datasets_of_interest = {"BHSig260-Hindi", "BHSig260-Bengali", "CEDAR"}

# Loop through each dataset in the root directory
for dataset in os.listdir(root_data_path):
    dataset_path = os.path.join(root_data_path, dataset, dataset)
    if dataset not in datasets_of_interest:
        continue
    # Loop through each folder in the dataset directory
    for folder in os.listdir(dataset_path):
        folder_path = os.path.join(dataset_path, folder)
        # Ensure the folder path is a directory
        if not os.path.isdir(folder_path):
            continue
        # Loop through each image in the folder
        for img in os.listdir(folder_path):
            img_path = os.path.join(folder_path, img)
            if dataset == "BHSig260-Hindi":
                # Check if the image is a forgery or an original
                if img.split("-")[3] == "F":
                    hindi_forgeries_imgs.append(img_path)
                else:
                    hindi_original_imgs.append(img_path)
            elif dataset == "BHSig260-Bengali":
                # Check if the image is a forgery or an original
                if img.split("-")[3] == "F":
                    bengali_forgeries_imgs.append(img_path)
                else:
                    bengali_original_imgs.append(img_path)
            elif dataset == "CEDAR":
                # Check if the image is a forgery or an original
                if img.split("_")[0] == "forgeries":
                    cedar_forgeries_imgs.append(img_path)
                else:
                    cedar_original_imgs.append(img_path)

# Combine all forgery and original image paths
all_forge_imgs = cedar_forgeries_imgs + hindi_forgeries_imgs + bengali_forgeries_imgs
all_org_imgs = cedar_original_imgs + hindi_original_imgs + bengali_original_imgs

# Print the counts for verification
print(f"\nBengali Forgeries: {len(bengali_forgeries_imgs)}, Originals: {len(bengali_original_imgs)}")
print(f"Hindi Forgeries: {len(hindi_forgeries_imgs)}, Originals: {len(hindi_original_imgs)}")
print(f"Cedar Forgeries: {len(cedar_forgeries_imgs)}, Originals: {len(cedar_original_imgs)}")
print(f"Total Forgeries: {len(all_forge_imgs)}, Total Originals: {len(all_org_imgs)}")

print(len(cedar_forgeries_imgs), len(cedar_original_imgs))

print(len(hindi_forgeries_imgs), len(hindi_original_imgs))

print(len(bengali_forgeries_imgs), len(bengali_original_imgs))

print(len(all_forge_imgs),len(all_org_imgs))

img = cv2.imread(all_forge_imgs[0])
img.shape

img = cv2.imread(all_org_imgs[0])
img.shape

"""merging all data

### Data Preprocessing

1.  Convert Image to grayscale: The image is first converted to grayscale to simplify the data and reduce the computational load. Grayscale images contain only shades of gray and no color information.
2.  Erosion followed by dilation: Erosion and dilation are morphological operations used to remove small noise and fill gaps in the image. Erosion erodes away the boundaries of the foreground object, and dilation enlarges the boundaries of the foreground object.
3.  GaussianBlur for enhancing edge details: GaussianBlur is applied to the grayscale image to reduce noise and enhance edge details. It replaces each pixel's value with the normalization value of its neighboring pixels, effectively reducing the impact of outliers or random noise.
4.  Binarization: After the erosion and dilation, the image is binarized to create a binary image with only two pixel values (usually 0 and 255). Binarization simplifies the image and helps in separating the foreground from the background.
5.  Cropping of signature: Finally, the preprocessed image is cropped to extract the signature or the region of interest from the rest of the image. Cropping focuses on the essential part of the image, which in this case is the signature.
"""

def preprocess_image(image_pth):

    gray = image_pth.convert("L")

    img = np.array(gray)

    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,2))
    morphology_img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=1)

    blur = cv2.GaussianBlur(morphology_img, (3,3),0)

    _, binary = cv2.threshold(blur, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    coords = cv2.findNonZero(binary)
    x, y, w, h = cv2.boundingRect(coords)

    padding = 5
    x -= padding
    y -= padding
    w += 2 * padding
    h += 2 * padding

    x = max(0, x)
    y = max(0, y)
    w = min(w, img.shape[1] - x)
    h = min(h, img.shape[0] - y)

    cropped_image = binary[y:y + h, x:x + w]

    extra_space = np.zeros((cropped_image.shape[0] + 2 * padding, cropped_image.shape[1] + 2 * padding), dtype=np.uint8) * 255
    extra_space[padding:-padding, padding:-padding] = cropped_image

    corrected = cv2.resize(extra_space,(330,175))
    resized_image = Image.fromarray(corrected)

    return resized_image

def plot_images(img):
    fig, axes = plt.subplots(1,2, figsize=(10,10))
    axes[0].imshow(img)
    axes[0].axis('off')
    axes[0].set_title("before_preprocessing")

    after_preprocessing = preprocess_image(img)
    print(after_preprocessing.size)
    axes[1].imshow(after_preprocessing)
    axes[1].axis('off')
    axes[1].set_title("after_preprocessing")

img = Image.open("Downloads/archive/BHSig260-Hindi/BHSig260-Hindi/1/H-S-1-F-01.tif")
plot_images(img)

img = Image.open('Downloads/archive/CEDAR/CEDAR/1/forgeries_1_1.png')
plot_images(img)

class AddGaussianNoise(object):
    def __init__(self, mean=0.0, std=0.1, p=0.5):
        self.mean = mean
        self.std = std
        self.p = p  # Probability of applying Gaussian noise

    def __call__(self, tensor):
        if torch.rand(1) < self.p:
            return tensor + torch.randn(tensor.size()) * self.std + self.mean
        return tensor

transformation = transforms.Compose([
    transforms.Resize((200,300)),
    transforms.ToTensor(),
    transforms.RandomRotation((-5,5)),
    AddGaussianNoise(mean=0.0, std=0.1,p=0.3),
])

def show_triplets_from_dataloader(dataloader, num_triplets=3):
    for batch_idx, (anchor_imgs, positive_imgs, negative_imgs) in enumerate(dataloader):
        for i in range(len(anchor_imgs)):
            if i >= num_triplets:
                break

            # Display the anchor, positive, and negative images
            fig, axes = plt.subplots(1, 3, figsize=(8, 4))
            axes[0].imshow(anchor_imgs[i].permute(1, 2, 0))  # Permute dimensions for image display (assuming tensor input)
            axes[0].set_title("Anchor")
            axes[1].imshow(positive_imgs[i].permute(1, 2, 0))
            axes[1].set_title("Positive")
            axes[2].imshow(negative_imgs[i].permute(1, 2, 0))
            axes[2].set_title("Negative")
            plt.tight_layout()
            plt.show()

"""### Triplet Dataset - Dataset with three pairs (Anchor, Positive, Negative)"""

def all_forge_img(data_path):

  forge_imgs = []
  for signature in os.listdir(data_path):
    signature_path = os.path.join(data_path, signature)
    for filename in os.listdir(signature_path):
      if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:
        forge_imgs.append(os.path.join(signature_path,filename))

  return forge_imgs

def triplet_dataset_preparation(data_path):

  all_forge_imgs = all_forge_img(data_path)

  df = pd.DataFrame(columns=['Anchor_Path', 'Positive_Path', 'Negative_Path'])

  for signature_folder in os.listdir(data_path):
    signature_folder_path = os.path.join(data_path, signature_folder)
    genuine_images = []
    forged_images = []
    for filename in os.listdir(signature_folder_path):
      if 'original' in filename or '-G-' in filename:
        genuine_images.append(os.path.join(signature_folder_path,filename))
      if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:
        forged_images.append(os.path.join(signature_folder_path,filename))

    forged_images.extend(random.sample(all_forge_imgs, 15))

    num_combinations = min(len(genuine_images) * (len(genuine_images) - 1) // 2, len(genuine_images) * len(forged_images))
    genuine_combinations = random.sample(list(itertools.combinations(genuine_images, 2)), num_combinations)
    forged_combinations = random.sample(list(itertools.product(genuine_images, forged_images)), num_combinations)

    data = []
    for (image_1, image_2), (genuine_image, forged_image) in zip(genuine_combinations, forged_combinations):
      anchor_path = os.path.join(image_1)
      positive_path = os.path.join(image_2)
      negative_path = os.path.join(forged_image)
      data.append([anchor_path, positive_path, negative_path])

    df_ext = pd.DataFrame(data, columns=['Anchor_Path', 'Positive_Path', 'Negative_Path'])

    df = pd.concat([df, df_ext], ignore_index=True)
  return df

"""### Dulet Dataset - Dataset with two pairs (Original, Forge)"""

def duplet_dataset_preparation(data_path):

  all_forge_imgs = all_forge_img(data_path)

  df = pd.DataFrame(columns=['Image1', 'Image2', 'Label'])

  for signature_folder in os.listdir(data_path):
    signature_folder_path = os.path.join(data_path, signature_folder)
    genuine_images = []
    forged_images = []
    for filename in os.listdir(signature_folder_path):
      if 'original' in filename or '-G-' in filename:
        genuine_images.append(os.path.join(signature_folder_path,filename))
      if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:
        forged_images.append(os.path.join(signature_folder_path, filename))

    forged_images.extend(random.sample(all_forge_imgs, 15))

    num_combinations = min(len(genuine_images) * (len(genuine_images) - 1) // 2, len(genuine_images) * len(forged_images))
    genuine_combinations = random.sample(list(itertools.combinations(genuine_images, 2)), num_combinations)
    forged_combinations = random.sample(list(itertools.product(genuine_images, forged_images)), num_combinations)

    data = []
    for (image_1, image_2), (genuine_image, forged_image) in zip(genuine_combinations, forged_combinations):
      anchor_path = os.path.join(image_1)
      positive_path = os.path.join(image_2)
      label = 1
      data.append([anchor_path, positive_path, label])

      anchor_path = os.path.join(genuine_image)
      positive_path = os.path.join(forged_image)
      label = 0
      data.append([anchor_path, positive_path, label])

    df_ext = pd.DataFrame(data, columns=['Image1', 'Image2', 'Label'])

    df = pd.concat([df, df_ext], ignore_index=True)

  return df

"""### Dataset Inference"""

import os
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset
import numpy as np

def all_forge_img(data_path):
    forge_imgs = []
    for signature in os.listdir(data_path):
        signature_path = os.path.join(data_path, signature)
        if not os.path.isdir(signature_path):
            continue
        for filename in os.listdir(signature_path):
            if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:
                forge_imgs.append(os.path.join(signature_path, filename))
    return forge_imgs

def triplet_dataset_preparation(data_path):
    all_forge_imgs = all_forge_img(data_path)

    # Assuming we have some logic to create anchor, positive, negative paths
    df = pd.DataFrame(columns=['Anchor_Path', 'Positive_Path', 'Negative_Path'])

    for signature_folder in os.listdir(data_path):
        folder_path = os.path.join(data_path, signature_folder)
        if not os.path.isdir(folder_path):
            continue
        # Some logic to create triplets
        # Here we just use placeholders, replace with actual logic
        anchor_path = os.path.join(folder_path, "anchor.jpg")
        positive_path = os.path.join(folder_path, "positive.jpg")
        negative_path = os.path.join(folder_path, "negative.jpg")
        df = df.append({'Anchor_Path': anchor_path, 'Positive_Path': positive_path, 'Negative_Path': negative_path}, ignore_index=True)

    return df

class TripletDataset(Dataset):
    def __init__(self, training_df=None, transform=None):
        self.training_df = training_df
        self.training_df.columns = ["Anchor_Path", "Positive_Path", "Negative_Path"]
        self.transform = transform

    def __getitem__(self, index):
        # Getting the image paths
        anchor_path = self.training_df.iat[int(index), 0]
        positive_path = self.training_df.iat[int(index), 1]
        negative_path = self.training_df.iat[int(index), 2]

        # Loading the images
        anchor_img = Image.open(anchor_path)
        positive_img = Image.open(positive_path)
        negative_img = Image.open(negative_path)

        # Preprocess the image
        anchor_img = preprocess_image(anchor_img)
        positive_img = preprocess_image(positive_img)
        negative_img = preprocess_image(negative_img)

        # Apply image transformations
        if self.transform is not None:
            anchor_img = self.transform(anchor_img)
            positive_img = self.transform(positive_img)
            negative_img = self.transform(negative_img)

        return anchor_img, positive_img, negative_img

    def __len__(self):
        return len(self.training_df)


class DupletDataset(Dataset):
    def __init__(self, dataframe=None, transform=None):
        # Used to prepare the labels and images path
        self.training_df = dataframe
        self.training_df.columns = ["image1", "image2", "label"]
        self.transform = transform

    def __getitem__(self, index):
        # Getting the image path
        image1_path = self.training_df.iat[int(index), 0]
        image2_path = self.training_df.iat[int(index), 1]

        # Loading the images
        img0 = Image.open(image1_path)
        img1 = Image.open(image2_path)

        # Preprocess the images
        img0 = preprocess_image(img0)
        img1 = preprocess_image(img1)

        # Apply image transformations
        if self.transform is not None:
            img0 = self.transform(img0)
            img1 = self.transform(img1)

        # Prepare the label
        label = torch.from_numpy(np.array([int(self.training_df.iat[int(index), 2])], dtype=np.float32))

        return img0, img1, label

    def __len__(self):
        return len(self.training_df)

import pandas as pd
import os
import itertools
import random

def all_forge_img(data_path):
    forge_imgs = []
    for signature in os.listdir(data_path):
        signature_path = os.path.join(data_path, signature)
        if not os.path.isdir(signature_path):
            continue
        for filename in os.listdir(signature_path):
            if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:
                forge_imgs.append(os.path.join(signature_path, filename))
    return forge_imgs

def triplet_dataset_preparation_debug(data_path):
    all_forge_imgs = all_forge_img(data_path)
    df = pd.DataFrame(columns=['Anchor_Path', 'Positive_Path', 'Negative_Path'])

    for signature_folder in os.listdir(data_path):
        signature_folder_path = os.path.join(data_path, signature_folder)
        if not os.path.isdir(signature_folder_path):
            continue
        genuine_images = []
        forged_images = []
        for filename in os.listdir(signature_folder_path):
            if 'original' in filename or '-G-' in filename:
                genuine_images.append(os.path.join(signature_folder_path, filename))
            if 'forgeries' in filename or '-F-' in filename or 'forge' in filename:
                forged_images.append(os.path.join(signature_folder_path, filename))

        if len(genuine_images) == 0:
            continue

        forged_images.extend(random.sample(all_forge_imgs, 15))
        num_combinations = min(len(genuine_images) * (len(genuine_images) - 1) // 2, len(genuine_images) * len(forged_images))
        genuine_combinations = random.sample(list(itertools.combinations(genuine_images, 2)), num_combinations)
        forged_combinations = random.sample(list(itertools.product(genuine_images, forged_images)), num_combinations)

        data = []
        for (image_1, image_2), (genuine_image, forged_image) in zip(genuine_combinations, forged_combinations):
            anchor_path = os.path.join(image_1)
            positive_path = os.path.join(image_2)
            negative_path = os.path.join(forged_image)
            data.append([anchor_path, positive_path, negative_path])

        df_ext = pd.DataFrame(data, columns=['Anchor_Path', 'Positive_Path', 'Negative_Path'])
        df = pd.concat([df, df_ext], ignore_index=True)

    # Print the DataFrame for debugging
    print("DataFrame shape:", df.shape)
    print("DataFrame head:\n", df.head())

    return df

# Create the DataFrame and check its contents
train_df_triplet = triplet_dataset_preparation_debug(train_path)
print(train_df_triplet.shape)
print(train_df_triplet.head())

# Sample triplet from the DataFrame
sample_triplet = train_df_triplet.iloc[0]
print(sample_triplet)



"""##### data for CNN"""

train_df_triplet.sample(10)

train_dataset =  TripletDataset(train_df_triplet, transform = transformation)

batch_size = 128
triplet_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= batch_size)

len(triplet_train_loader)

num_samples_to_show = 6

# Get random indices to select samples from the DataLoader
num_samples = len(triplet_train_loader.dataset)
random_indices = np.random.choice(num_samples, num_samples_to_show, replace=False)

# Create a SubsetRandomSampler using the random indices
sampler = SubsetRandomSampler(random_indices)

# Create a new DataLoader using the SubsetRandomSampler
random_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)

# Show the random samples
show_triplets_from_dataloader(random_loader, num_samples_to_show)

len(train_loader)

"""## CNN Model Pipeline

### Model classes
"""

class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc1 = nn.Conv2d(in_channels, in_channels // reduction_ratio, kernel_size=1)
        self.relu = nn.ReLU()
        self.fc2 = nn.Conv2d(in_channels // reduction_ratio, in_channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_pool = self.avg_pool(x)
        max_pool = self.max_pool(x)
        avg_out = self.fc2(self.relu(self.fc1(avg_pool)))
        max_out = self.fc2(self.relu(self.fc1(max_pool)))
        attention = self.sigmoid(avg_out + max_out)
        return x * attention


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        pool = torch.cat([avg_pool, max_pool], dim=1)
        attention = self.sigmoid(self.conv(pool))
        return x * attention


class CBAM(nn.Module):
    def __init__(self, in_channels, reduction_ratio=8, spatial_kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)
        self.spatial_attention = SpatialAttention(spatial_kernel_size)

    def forward(self, x):
        x = self.channel_attention(x)
        x = self.spatial_attention(x)
        return x


class SiameseResNet(nn.Module):
    def __init__(self, model_name='resnet18', pretrained=False):
        super(SiameseResNet, self).__init__()
        self.baseModel = models.resnet18(pretrained=pretrained)

        # Experiment with different spatial sizes based on the image resolution and signature complexity
        self.attention1 = CBAM(in_channels=64)  # CBAM for layer 1
        self.attention2 = CBAM(in_channels=256)  # CBAM for layer 3

        self.baseModel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)

        # Remove the fully connected layer
        self.baseModel.fc = nn.Identity()

    def forward(self, x):
        out = self.baseModel.conv1(x)
        out = self.baseModel.bn1(out)
        out = self.baseModel.relu(out)
        out = self.baseModel.maxpool(out)

        out = self.attention1(self.baseModel.layer1(out))  # Applying CBAM to layer 1
        out = self.baseModel.layer2(out)
        out = self.attention2(self.baseModel.layer3(out))  # Applying CBAM to layer 3
        out = self.baseModel.layer4(out)

        # Global Average Pooling (GAP)
        out = F.adaptive_avg_pool2d(out, (1, 1))  # Perform GAP to reduce spatial dimensions to 1x1
        out = torch.flatten(out, 1)
        return out

class TriangularMarginLoss(nn.Module):
    def __init__(self, margin=0.5):
        super(TriangularMarginLoss, self).__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative):
        distance_anchor_positive = F.pairwise_distance(anchor, positive, p=2)
        distance_anchor_negative = F.pairwise_distance(anchor, negative, p=2)

        # Triangular Margin Loss
        loss = torch.relu(distance_anchor_positive - distance_anchor_negative + self.margin)

        return loss.mean()

def train_model(model, loader, n_epochs, optimizer, loss_fn):
    history = {'train_loss': []}
    print("------------------------Training--------------------------")
    for epoch in range(1, n_epochs + 1):
        t0 = datetime.now()
        print(f"Beginning Epoch {epoch}/{n_epochs}...")
        train_loss = []
        model.train()
        for i, data in tqdm(enumerate(loader, 0)):
            anchor, positive, negative = data
            anchor = anchor.to(device=device)
            positive = positive.to(device=device)
            negative = negative.to(device=device)

            optimizer.zero_grad()
            anchor_embeddings = model(anchor)
            positive_embeddings = model(positive)
            negative_embeddings = model(negative)

            loss = loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)  # Changed `triplet_loss` to `loss_fn`

            loss.backward()
            optimizer.step()
            train_loss.append(loss.item())  # Added the loss value to `train_loss` list

        dt = datetime.now() - t0
        print('\nEpoch: {}\tTrain Loss: {}\tDuration: {}'.format(epoch, np.mean(train_loss), dt))

        # Tracking loss in each epoch for plot
        history['train_loss'].append(np.mean(train_loss))

    return history

"""### Training Model"""

margin = 0.1

# Create an instance of SiameseResnet with the ResNet model and embedding size
siamese_model = SiameseResNet()
siamese_model = nn.DataParallel(siamese_model).to(device)

triplet_loss = TriangularMarginLoss(margin).to(device)
optimizer = torch.optim.Adam(siamese_model.parameters(), lr=0.001)

num_epochs = 10

history = train_model(siamese_model, triplet_train_loader, num_epochs, optimizer, triplet_loss)

torch.save(siamese_model.state_dict(),'Downloads/archives/Signature_forgery_detection_model')

plt.plot(range(1,11), history['train_loss'], 'b', label='Triplet Loss')
plt.title('Triplet Loss of Model', fontsize=16)
plt.xlabel('Epochs', fontsize=14)
plt.ylabel('Loss', fontsize=14)
plt.legend()

"""### Model Inference"""

def get_similarity(img1, img2):
    fig, axes = plt.subplots(1,2, figsize=(8,8))
    # Load the images
    image1 = Image.open(img1)
    image1 = preprocess_image(image1)
    axes[0].imshow(image1)
    axes[0].axis('off')
    axes[0].set_title("Genuine Image")

    image2 = Image.open(img2)
    image2 = preprocess_image(image2)
    axes[1].imshow(image2)
    axes[1].axis('off')
    axes[1].set_title("Test Image")


    # Preprocess the images
    transform = transforms.Compose([
        transforms.Resize((175,330)),
        transforms.ToTensor(),
    ])
    input1 = transform(image1).unsqueeze(0).to(device)
    input2 = transform(image2).unsqueeze(0).to(device)

    # Make predictions using the model
    siamese_model.eval()
    with torch.no_grad():
        pred1 = siamese_model(input1)
        pred2 = siamese_model(input2)

        dist = torch.pairwise_distance(pred1, pred2)
        similarity_score = 1 / (1 + dist)

        print(similarity_score)

img1 = 'Downloads/archives/CEDAR/CEDAR/1/original_1_2.png'
img2 = 'Downloads/archives/CEDAR/CEDAR/1/original_1_3.png'
get_similarity(img1, img2)

img1 = 'Downloads/archives/CEDAR/CEDAR/1/forgeries_1_1.png'
img2 = 'Downloads/archives/CEDAR/CEDAR/1/original_1_3.png'
get_similarity(img1, img2)

img1 = 'Downloads/archives/CEDAR/CEDAR/2/forgeries_2_1.png'
img2 = 'Downloads/archives/CEDAR/CEDAR/1/original_1_3.png'
get_similarity(img1, img2)

"""## Model Testing and Results"""

def calculate_accuracy(data_loader, model):
    model.eval()  # Set the model to evaluation mode
    correct = 0
    total = 0

    with torch.no_grad():
        for data in data_loader:
            inputs1, inputs2, targets = data
            inputs1 = inputs1.to(device)
            inputs2 = inputs2.to(device)
            targets = targets.to(device)

            outputs = model(inputs1, inputs2)
            predictions = (outputs > 0.5).float()  # Convert probabilities to binary predictions
            correct += (predictions == targets).sum().item()
            total += targets.size(0)

    accuracy = correct / total * 100
    return accuracy

def get_predictions(data_loader, model):
    all_preds = []
    all_labels = []
    model.eval()
    for i, data in tqdm(enumerate(data_loader, 0)):
        inputs1, inputs2, targets = data
        inputs1 = inputs1.to(device=device)
        inputs2 = inputs2.to(device=device)
        targets = targets.to(device=device)

        output = model(inputs1, inputs2)
        predictions = (output > 0.5).float().tolist()  # Convert predictions to list format
        all_preds.extend(predictions)
        all_labels.extend(targets.tolist())

    return all_preds, all_labels

def build_confusion_matrix(preds, labels):
    # Build confusion matrix
    cm = confusion_matrix(preds, labels)

    # Normalize the confusion matrix
    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

    # Define the labels
    labels = ['Genuine', 'Forged']
    # Plot the confusion matrix
    sns.heatmap(cm_percentage, annot=True, fmt='.1f', cmap='Blues')
    plt.gca().set_xticklabels(labels, rotation=0)
    plt.gca().set_yticklabels(labels, rotation=0)
    # Set the axis labels and title
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Confusion Matrix (Percentage)')

    # Display the plot
    plt.show()

"""##### confusion matrix for validation data"""

preds, labels = get_predictions(validation_loader, model_rms)
print(classification_report(preds, labels))
build_confusion_matrix(preds, labels)

"""##### confusion matrix for test data"""

preds, labels = get_predictions(test_loader, model_rms)
print(classification_report(preds, labels))
build_confusion_matrix(preds, labels)

"""##### confusion matrix for Hindi Data"""

cedar_dataset = 'Downloads/archives/BHSig260-Hindi/BHSig260-Hindi'
cedar_duplet = duplet_dataset_preparation(cedar_dataset)

cedar_duplet = cedar_duplet.sample(12000, random_state=42)

transformation = transforms.Compose([
    transforms.Resize((200,300)),
#     transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
])
cedar_dataset = DupletDataset(cedar_duplet, transform=transformation)

validation_loader = torch.utils.data.DataLoader(
    cedar_dataset, batch_size=32
)
preds, labels = get_predictions(validation_loader, model_rms)
print(classification_report(preds, labels))
build_confusion_matrix(preds, labels)

"""## Inference Pipeline"""

def get_predictions_for_single(img1, img2, model):
    # Load the images
    image1 = Image.open(img1)
    plt.imshow(image1)
    plt.show()
    image1 = preprocess_image(image1)
    image2 = Image.open(img2)
    plt.imshow(image2)
    plt.show()
    image2 = preprocess_image(image2)



    # Preprocess the images
    transform = transforms.Compose([
        transforms.Resize((200,300)),
#         transforms.Grayscale(num_output_channels=3),
        transforms.ToTensor(),
    ])
    input1 = transform(image1).unsqueeze(0).to(device)
    input2 = transform(image2).unsqueeze(0).to(device)

    # Make predictions using the model
    model.eval()
    with torch.no_grad():
        predictions = model(input1, input2)
        print(predictions)

    # Determine the prediction label
    if predictions > 0.8:
        print("The signatures are similar.")
    else:
        print("The signatures are not similar.")

img1 = "Downloads/archives/CEDAR/CEDAR/11/original_11_2.png"
img2 = "Downloads/archives/CEDAR/CEDAR/11/original_11_5.png"
get_predictions_for_single(img1, img2, model_rms)

img1 = "Downloads/archives/CEDAR/CEDAR/11/original_11_3.png"
img2 = "Downloads/archives/CEDAR/CEDAR/2/original_2_3.png"
get_predictions_for_single(img1, img2, model_rms)

img1 = "Downloads/archives/CEDAR/CEDAR/11/original_11_3.png"
img2 = "Downloads/archives/CEDAR/CEDAR/11/forgeries_11_3.png"
get_predictions_for_single(img1, img2, model_rms)

"""# Conclusion

The provided code demonstrates the process of training and evaluating a Logistic Siamese Regression (LSR) model for signature forgery detection. The model takes pairs of signature images as input and outputs a probability score between 0 and 1, where a score closer to 1 indicates that the signatures are likely to be genuine, and a score closer to 0 indicates that they are likely to be forged.

The code snippet includes the following steps:

1. **Dataset Preparation:**
    - The code defines a `triplet_dataset_preparation` function that takes a directory containing signature images as input and generates a df of tri-pairs of genuine, another different genuine and forged signatures. It then creates group of images, ensuring that each group consists of one genuine, one different agle of genuine and one forged signature.
    - The code defines a `duplet_dataset_preparation` function that takes a directory containing signature images as input and generates a df of pairs of genuine and forged signatures. It then creates pairs of images, ensuring that each pair consists of one genuine and one forged signature.

2. **Data Transformation and Preprocessing:**
    - The code defines a `transformation` object that applies a series of transformations to the signature images.
    - The transformations include resizing the images to a specific size and converting them to tensors, Random Rotation and Adding gausian blur.
    - In data preprocessing we doing: Convert Image to grayscale, GausianBlur, Binarization, Erosion, Dilation, Croping.

3. **Data Loading:**
    - The code uses the `torch.utils.data.DataLoader` class to create data loaders for training, validation, and testing.
    - The data loaders batch the data and shuffle it for efficient training and evaluation.

4. **Model Training:**
    - First we fine-tuned (transfer learning) a ResNet18 pretrained model with CEDAR dataset with CBAM integration.
    - Then the `LogisticSiameseRegression` class, which combines a trained Siamese model with a logistic regression classifier.
    - The model is trained on a dataset of genuine and forged signature pairs using the Adam optimizer and the binary cross-entropy loss function.
    - The training process is monitored using training and validation accuracy and loss metrics.

5. **Model Evaluation:**
    - The trained model is evaluated on the validation and test datasets to assess its performance.
    - The code calculates the classification accuracy and builds confusion matrices to visualize the model's predictions.
    - The results show that the model achieves high accuracy on both the validation and test datasets.

6. **Inference Pipeline:**
    - The code defines a function `get_predictions_for_single` that takes two signature images as input and outputs a prediction on their authenticity.
    - The function preprocesses the images, passes them through the trained model, and interprets the output probability score to determine if the signatures are genuine or forged.


The CNN (ResNet18 + CBAM) is converging well, but underfitting due to limited epochs and GPU constraints (free Kaggle). This affects LRN's performance (overfitting).

**To improve:**

    - Explore data augmentation (e.g., .tif files)
    - Tune hyperparameters (if time permits)
    - Consider deeper CNN (ResNet50) for better results
    - The model can still distinguish forged vs. original signs using inference.

Focus on increasing epochs/data augmentation first. If resources allow, experiment with ResNet50. Make changes incrementally, evaluate, and adjust. Consider regularization to prevent overfitting.

**Val Accuracy = 76%**

Can Detect forge and original Sign if we provide two images directly using Infernece function.

Overall, the provided code demonstrates the effectiveness of using a Logistic Siamese Regression model for signature forgery detection. The model can be used to analyze pairs of signature images and provide an assessment of their authenticity.
"""